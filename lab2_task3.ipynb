{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01730ec2",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c767330",
   "metadata": {},
   "source": [
    "**Создаем сессию с заданным конфигом**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85306f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-SVGHN8C:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1670447623850)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 00:13:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession, Row}\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3fe2e390\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 00:13:55 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession, Row}\n",
    "\n",
    "//Пример 1 запуска\n",
    "val spark = SparkSession.builder\n",
    "        //.config(\"spark.jars\", \"/home/.../spark/postgresql-42.2.20.jar\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8356943",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "**Читаем паркеты с данными**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6722fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 9 more fields]\r\n",
       "countries: org.apache.spark.sql.DataFrame = [country_id: string, country_name: string ... 1 more field]\r\n",
       "departments: org.apache.spark.sql.DataFrame = [department_id: int, department_name: string ... 2 more fields]\r\n",
       "jobs: org.apache.spark.sql.DataFrame = [job_id: string, job_title: string ... 2 more fields]\r\n",
       "job_history: org.apache.spark.sql.DataFrame = [employee_id: int, start_date: string ... 3 more fields]\r\n",
       "locations: org.apache.spark.sql.DataFrame = [location_id: int, street_address: string ... 4 more fields]\r\n",
       "regions: org.apache.spark.sql.DataFrame = [region_id: int, region_name: string]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\employees.parquet\")\n",
    "val countries = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\countries.parquet\")\n",
    "val departments = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\department.parquet\")\n",
    "val jobs = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\jobs.parquet\")\n",
    "val job_history = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\job_history.parquet\")\n",
    "val locations = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\locations.parquet\")\n",
    "val regions = spark.read.parquet(\"D:\\\\BD_MEPHI\\\\lab1\\\\data\\\\regions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb15cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id| salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17.06.2003|AD_PRES|24000.0|          null|      null|           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21.09.2005|  AD_VP|17000.0|          null|       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13.01.2001|  AD_VP|17000.0|          null|       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03.01.2006|IT_PROG| 9000.0|          null|       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21.05.2007|IT_PROG| 6000.0|          null|       103|           60|\n",
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "employees.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730663a9",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------\n",
    "**Импортируем Window, создаем строковую переменную с датой** <br>\n",
    "(с типом Date адекватно сделать не получилось, поэтому далее костыли через unix timestamp)<br>\n",
    "\n",
    "**Inner Join'им необходимые DF. Inner, а не Left, потому что в задании нужно вывести данные по городам, а null - это не город** <br>\n",
    "\n",
    "**В итоге из employees получаем employees_extended - DF, где собраны только нужные нам столбцы** <br>\n",
    "\n",
    "**Далее уже на нем используем оконную функцию. Используем сортировку по убыванию на столбце total. Нумеруем строки, чтобы потом фильтром взять только первую из каждого города (ту, где total больше всего среди всех строк, относящихся к этому городу)**<br>\n",
    "\n",
    "**Выводим результат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f5f86969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "|               city|employee_id|hire_date_casted|months_diff| salary|    total|\n",
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "|             London|        203|      2002-06-07|        150| 6500.0| 975000.0|\n",
      "|             Munich|        204|      2002-06-07|        150|10000.0|1500000.0|\n",
      "|             Oxford|        145|      2004-10-01|        123|14000.0|1722000.0|\n",
      "|            Seattle|        100|      2003-06-17|        138|24000.0|3312000.0|\n",
      "|South San Francisco|        122|      2003-05-01|        140| 7900.0|1106000.0|\n",
      "|          Southlake|        103|      2006-01-03|        107| 9000.0| 963000.0|\n",
      "|            Toronto|        201|      2004-02-17|        130|13000.0|1690000.0|\n",
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n",
       "date: String = 2015-01-01\r\n",
       "employees_extended: org.apache.spark.sql.DataFrame = [city: string, employee_id: int ... 4 more fields]\r\n",
       "window: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@97ecad\r\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "var date = \"2015-01-01\"\n",
    "\n",
    "val employees_extended = employees\n",
    "    .join(departments, departments(\"department_id\") === employees(\"department_id\"), \"inner\")\n",
    "    .join(locations, locations(\"location_id\") === departments(\"location_id\"), \"inner\")\n",
    "    .withColumn(\"set_date_string\", lit(date))\n",
    "    .withColumn(\"set_date\", to_date(from_unixtime(unix_timestamp(col(\"set_date_string\"), \"yyyy-mm-dd\"), \"yyyy-mm-dd\")))\n",
    "    .withColumn(\"hire_date_casted\", to_date(from_unixtime(unix_timestamp(col(\"hire_date\"), \"dd.mm.yyyy\"), \"yyyy-mm-dd\")))\n",
    "    .withColumn(\"months_diff\", floor(months_between(col(\"set_date\"), col(\"hire_date_casted\"))))\n",
    "    .withColumn(\"total\", col(\"salary\") * col(\"months_diff\"))\n",
    "    .select(\n",
    "        col(\"city\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"hire_date_casted\"),\n",
    "        col(\"months_diff\"),\n",
    "        col(\"salary\"),\n",
    "        col(\"total\")\n",
    "    )\n",
    "\n",
    "//employees_extended.show(10)\n",
    "\n",
    "val window  = Window.partitionBy(\"city\").orderBy(col(\"total\").desc)\n",
    "\n",
    "employees_extended\n",
    "    .withColumn(\"row\", row_number.over(window))\n",
    "    .filter(col(\"row\") === 1)\n",
    "    .select(\n",
    "        col(\"city\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"hire_date_casted\"),\n",
    "        col(\"months_diff\"),\n",
    "        col(\"salary\"),\n",
    "        col(\"total\")\n",
    "    )\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054103c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
