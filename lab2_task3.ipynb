{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01730ec2",
   "metadata": {},
   "source": [
    "# Задание 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c767330",
   "metadata": {},
   "source": [
    "**Создаем сессию с заданным конфигом**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d85306f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession, Row\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8356943",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Читаем паркеты с данными**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6722fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = spark.read.parquet(\"data/employees.parquet\")\n",
    "job_history = spark.read.parquet(\"data/job_history.parquet\")\n",
    "departments = spark.read.parquet(\"data/department.parquet\")\n",
    "jobs = spark.read.parquet(\"data/jobs.parquet\")\n",
    "locations = spark.read.parquet(\"data/locations.parquet\")\n",
    "countries = spark.read.parquet(\"data/countries.parquet\")\n",
    "regions = spark.read.parquet(\"data/regions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bb15cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id| salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17.06.2003|AD_PRES|24000.0|          null|      null|           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21.09.2005|  AD_VP|17000.0|          null|       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13.01.2001|  AD_VP|17000.0|          null|       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03.01.2006|IT_PROG| 9000.0|          null|       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21.05.2007|IT_PROG| 6000.0|          null|       103|           60|\n",
      "+-----------+----------+---------+--------+------------+----------+-------+-------+--------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "730663a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Импортируем Window, создаем строковую переменную с датой** <br>\n",
    "(с типом Date адекватно сделать не получилось, поэтому далее костыли через unix timestamp)<br>\n",
    "\n",
    "**В итоге из employees получаем employees_extended - DF, где собраны только нужные нам столбцы** <br>\n",
    "\n",
    "**Далее уже на нем используем оконную функцию. Используем сортировку по убыванию на столбце total. Нумеруем строки, чтобы потом фильтром взять только первую из каждого города (ту, где total больше всего среди всех строк, относящихся к этому городу)**<br>\n",
    "\n",
    "**Выводим результат**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f86969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "|               city|employee_id|hire_date_casted|months_diff| salary|    total|\n",
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "|             London|        203|      2002-06-07|        150| 6500.0| 975000.0|\n",
      "|             Munich|        204|      2002-06-07|        150|10000.0|1500000.0|\n",
      "|             Oxford|        145|      2004-10-01|        123|14000.0|1722000.0|\n",
      "|            Seattle|        100|      2003-06-17|        138|24000.0|3312000.0|\n",
      "|South San Francisco|        122|      2003-05-01|        140| 7900.0|1106000.0|\n",
      "|          Southlake|        103|      2006-01-03|        107| 9000.0| 963000.0|\n",
      "|            Toronto|        201|      2004-02-17|        130|13000.0|1690000.0|\n",
      "+-------------------+-----------+----------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "date = \"2015-01-01\"\n",
    "\n",
    "employees_extended = (\n",
    "    employees.join(departments, [\"department_id\"])\n",
    "    .join(locations, [\"location_id\"])\n",
    "    .withColumn(\"set_date_string\", lit(date))\n",
    "    .withColumn(\n",
    "        \"set_date\",\n",
    "        to_date(\n",
    "            from_unixtime(\n",
    "                unix_timestamp(col(\"set_date_string\"), \"yyyy-mm-dd\"), \"yyyy-mm-dd\"\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"hire_date_casted\",\n",
    "        to_date(\n",
    "            from_unixtime(unix_timestamp(col(\"hire_date\"), \"dd.mm.yyyy\"), \"yyyy-mm-dd\")\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"months_diff\", floor(months_between(col(\"set_date\"), col(\"hire_date_casted\")))\n",
    "    )\n",
    "    .withColumn(\"total\", col(\"salary\") * col(\"months_diff\"))\n",
    "    .select(\n",
    "        col(\"city\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"hire_date_casted\"),\n",
    "        col(\"months_diff\"),\n",
    "        col(\"salary\"),\n",
    "        col(\"total\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "window = Window.partitionBy(\"city\").orderBy(col(\"total\").desc())\n",
    "\n",
    "(\n",
    "    employees_extended.withColumn(\"row\", row_number().over(window))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .select(\n",
    "        col(\"city\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"hire_date_casted\"),\n",
    "        col(\"months_diff\"),\n",
    "        col(\"salary\"),\n",
    "        col(\"total\"),\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054103c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b389a781dfbb23638dad46b0ca61526fcae620ed023e26de210f45ecc9d3123a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
